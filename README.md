# AI-ML-Adverserial-Attacks
AI/ML Security

## AI/ML Security

## Perturbation
It is a technique of input modification in such a way so that original input retains the semantics but fools the AI/ML model prediction 

## Image Perturbation
In this notebook we will create a Image classifier and later try to perturb (modify) an input image by adding some noise without changing much of its appearnce and fool the classifier to perform incorrect prediction. The technique will be used is FGSM. For more detail on FGSM here is the link to the paper https://arxiv.org/abs/1412.6572 

Model is deployed as tensorflow js

https://vinaybist.github.io/AI-ML-Adverserial-Attacks

https://vinaybist.github.io/AI-ML-Adverserial-Attacks
